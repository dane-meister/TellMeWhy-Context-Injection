{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 354 - Nxt Lvl Programmars (Final Project)\n",
        "\n"
      ],
      "metadata": {
        "id": "7P2tmwgfEFbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Description: TellMeWhy with Small Models: Fine-Tuning Through Contextual Injection\n",
        "### Authors: Jaret McManus, Dane Meister\n",
        "Project Number: 9 Project Name: Answering Why Questions in TellMeWhy Project Area: Brief 2 line Project Description (starting point): Given a short story and a why question about an action in the story, generate an answer that explains the reason for performing the action. Relevant Baseline Model: T5, GPT, Gemini Relevant Dataset: TellMeWhy Relevant Papers: Lal et al 2021, Raffel et al 2020, Brown et al 2020, Gemini Team 2023\n",
        "\n",
        "\n",
        "**Readme**: [Gdrive link to readme](https://drive.google.com/file/d/1Dnp3O8TfjzQ75mYuFVYoyuBFkl9e1IrX/view?usp=drive_link)"
      ],
      "metadata": {
        "id": "LAmtDrdLEHWF"
      }
    },
    {
      "source": [
        "# upload and display README for project if necessary\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # This will prompt you to upload the file\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Get the filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]  # Assuming only the readme file is uploaded\n",
        "\n",
        "# Now open the file using the filename\n",
        "with open(filename, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "display(Markdown(content))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yc4H7bWamBOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Loading Google Drive*"
      ],
      "metadata": {
        "id": "sCzTEUWkEZIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmDIBrAHECzD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/CSE_354_Project'\n",
        "drive.mount('/content/drive/')\n",
        "if not os.path.exists(base_dir):\n",
        "    print(f\"Directory '{base_dir}' does not exist. Creating it...\")\n",
        "    os.makedirs(base_dir)\n",
        "else:\n",
        "    print(f\"Directory '{base_dir}' already exists.\")\n",
        "\n",
        "%cd $base_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Loading Dataset with context injected data*"
      ],
      "metadata": {
        "id": "HChoa2FUEpEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We injected the data with additional context using Google's Gemini LLM. For each unique narrative, we prompted Gemini to generate commonsense and external context, and saved it to a JSON file, to use later. We were limited by Google's API limits, so we modidfied the data in chunks, and later recombined all the chunks into one JSON file.\n",
        "\n",
        "For more details into how we did this, here is a link to the 2 notebooks containing the code:\n",
        "- [Notebook for Injecting Context in Chunks](https://colab.research.google.com/drive/1S50O26o_tLbaYE2-s-hRelSfroXaaMaC?usp=sharing)\n",
        "- [Notebook for Combining Chunks](https://colab.research.google.com/drive/1-o8IBF1KQgMBm7m_2hQVA83sE-cPh-yU?usp=sharing)"
      ],
      "metadata": {
        "id": "u02qepoAGRAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "4yvQIi2qEyae",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load our data\n",
        "file_name = 'ALL_CONTEXT_DATA_1.json'\n",
        "with open(file_name, 'r') as f:\n",
        "    all_context_data = json.load(f)\n",
        "print(f\"Loaded {len(all_context_data)} records from {file_name}\")"
      ],
      "metadata": {
        "id": "6Y3c-c8_E2bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Preprocess Data for Transformer*"
      ],
      "metadata": {
        "id": "Pr0yzmMrtduj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import T5Tokenizer"
      ],
      "metadata": {
        "id": "o0DtwOwmtiII",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the input\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"input\"],\n",
        "        max_length=128 ,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"target\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    ).input_ids\n",
        "\n",
        "    # Replace padding token IDs in labels with -100\n",
        "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels #add to dictionary\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Convert data into Dataset format\n",
        "context_dataset = Dataset.from_dict({\n",
        "    \"input\": [f\"Narrative: {item['narrative']} Context: {item['context']} Question: {item['question']}\" for item in all_context_data],\n",
        "    \"target\": [item['answer'] for item in all_context_data],\n",
        "})\n",
        "print(f\"Context injected Dataset loaded with {len(context_dataset)} samples.\")\n",
        "\n",
        "no_context_dataset = Dataset.from_dict({\n",
        "    \"input\": [f\"Narrative: {item['narrative']} Question: {item['question']}\" for item in all_context_data],\n",
        "    \"target\": [item['answer'] for item in all_context_data],\n",
        "})\n",
        "print(f\"No Context injected Dataset loaded with {len(no_context_dataset)} samples.\")\n",
        "\n",
        "\n",
        "# Initialize the T5 tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Apply tokenization to the dataset\n",
        "print(\"Tokenizing the dataset...\")\n",
        "tokenized_context_dataset = context_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_no_context_dataset = no_context_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# View tokenized example for verification\n",
        "print(\"Tokenized example:\", tokenized_context_dataset[0])\n",
        "print(\"Tokenized example:\", tokenized_no_context_dataset[100])"
      ],
      "metadata": {
        "id": "S5ZkSLrEUdy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Set up data for training*"
      ],
      "metadata": {
        "id": "_79a7aZoynjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_context_dataset = tokenized_context_dataset.select(range(10_000)).train_test_split(test_size=0.15)\n",
        "split_no_context_dataset = tokenized_no_context_dataset.select(range(10_000)).train_test_split(test_size=0.15)\n",
        "\n",
        "train_context_dataset = split_context_dataset[\"train\"]\n",
        "test_context_dataset = split_context_dataset[\"test\"]\n",
        "\n",
        "train_no_context_dataset = split_no_context_dataset[\"train\"]\n",
        "test_no_context_dataset = split_no_context_dataset[\"test\"]\n",
        "\n",
        "print(\"length of context train:\", len(train_context_dataset))\n",
        "print(\"length of context test:\", len(test_context_dataset))\n",
        "print(\"length of no context train:\", len(train_no_context_dataset))\n",
        "print(\"length of no context test:\", len(test_no_context_dataset))"
      ],
      "metadata": {
        "id": "BbERuh315VGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Load BLEURT for metric*"
      ],
      "metadata": {
        "id": "0d5OZGjnyFOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "import evaluate\n",
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "bleurt = evaluate.load(\"bleurt\")"
      ],
      "metadata": {
        "id": "MsMS24sJyI7P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "def compute_metrics(eval_pred):\n",
        "    print('eval')\n",
        "    logits, label_ids = eval_pred.predictions, eval_pred.label_ids\n",
        "    logits = logits[0]\n",
        "    with torch.no_grad():\n",
        "        # Convert logits to predictions (use argmax to get the most probable token)\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Compute BLEURT scores for the current batch\n",
        "        scores = bleurt.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "        del logits, label_ids, preds, decoded_preds, decoded_labels\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()  # Free unused GPU memory\n",
        "\n",
        "        # Return the average BLEURT score across all batches\n",
        "        return {\"bleurt\": np.mean(scores[\"scores\"])}"
      ],
      "metadata": {
        "id": "wlFMwaCG55OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Retrieve pretrained T5 model to finetune without context*"
      ],
      "metadata": {
        "id": "rIIUmC-WwjzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load the T5 model\n",
        "t5 = model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")  # Change to a larger version if needed"
      ],
      "metadata": {
        "id": "zn2hcXm3xbGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Finetuning a T5 without context*"
      ],
      "metadata": {
        "id": "qsEMwk5Xw9Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",         # Output directory\n",
        "    eval_strategy=\"epoch\",   # Evaluate every epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=1, # Adjust based on memory\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=4,            # Adjust based on performance\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,            # Save only the best checkpoint\n",
        "    logging_dir=\"./logs\",          # Directory for logs\n",
        "    logging_steps=60,\n",
        "    save_steps=1000,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    eval_accumulation_steps=8\n",
        ")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_no_context_dataset,\n",
        "    eval_dataset=test_no_context_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "trainer.save_model(\"./fine_tuned_whitespace_no_context\")\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "On4p3dAesAPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('./fine_tuned_whitespace_no_context', base_dir+'/fine_tuned_whitespace_no_context')"
      ],
      "metadata": {
        "id": "QmDEKFvm4YCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for conserving credits\n",
        "# leave training running, and disconnect when it finishes\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "7uo6fj-h4hl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Retreive a pretrained T5 to finetune with context*"
      ],
      "metadata": {
        "id": "J8pQEICpxNwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Reload the T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")  # Change to a larger version if needed"
      ],
      "metadata": {
        "id": "kAxBz29dZqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Finetuning a T5 with context*"
      ],
      "metadata": {
        "id": "aaKhk76Nxi3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",         # Output directory\n",
        "    eval_strategy=\"epoch\",   # Evaluate every epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4, # Adjust based on memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=4,            # Adjust based on performance\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,            # Save only the best checkpoint\n",
        "    logging_dir=\"./logs\",          # Directory for logs\n",
        "    logging_steps=60,\n",
        "    save_steps=1000,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    eval_accumulation_steps=8\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_context_dataset,\n",
        "    eval_dataset=test_context_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "trainer.save_model(\"./fine_tuned_whitespace_context\")\n",
        "trainer.evaluate()\n",
        "\n"
      ],
      "metadata": {
        "id": "lAv8VY3BaEKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('./fine_tuned_whitespace_no_context', base_dir+'/fine_tuned_whitespace_no_context')"
      ],
      "metadata": {
        "id": "CQSGEKAZ1fj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for conserving credits\n",
        "# leave training running, and disconnect when it finishes\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "_2HuHRbw1jvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Evaluating models*"
      ],
      "metadata": {
        "id": "YroKHWEVxnfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load models from google drive"
      ],
      "metadata": {
        "id": "pLwO2OcbLSUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_model = T5ForConditionalGeneration.from_pretrained(f\"{base_dir}/fine_tuned_whitespace_context\")\n",
        "no_context_model = T5ForConditionalGeneration.from_pretrained(f\"{base_dir}/fine_tuned_whitespace_no_context\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "85LH0n6vkpZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload our data in case of starting colab at this point"
      ],
      "metadata": {
        "id": "XBLp536YND7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load our data\n",
        "file_name = 'ALL_CONTEXT_DATA_1.json'\n",
        "with open(file_name, 'r') as f:\n",
        "    all_context_data = json.load(f)\n",
        "print(f\"Loaded {len(all_context_data)} records from {file_name}\")"
      ],
      "metadata": {
        "id": "3UEwVZhFMDKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get smaller subset to evaluate on\n",
        "no_context_dataset = Dataset.from_dict({\n",
        "    \"input\": [f\"Narrative: {item['narrative']} Question: {item['question']}\" for item in all_context_data],\n",
        "    \"target\": [item['answer'] for item in all_context_data],\n",
        "})\n",
        "context_dataset = Dataset.from_dict({\n",
        "    \"input\": [f\"Narrative: {item['narrative']} Context: {item['context']} Question: {item['question']}\" for item in all_context_data],\n",
        "    \"target\": [item['answer'] for item in all_context_data],\n",
        "})\n",
        "context_dataset = context_dataset.shuffle(seed=42).select(range(1_000))\n",
        "print(context_dataset[0])\n",
        "no_context_dataset = no_context_dataset.shuffle(seed=42).select(range(1_000))\n",
        "print(no_context_dataset[0])"
      ],
      "metadata": {
        "id": "_y8EgFcgk-5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install tqdm\n",
        "import evaluate\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0Dud9FRUlqCO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bleurt_metric = evaluate.load(\"bleurt\")"
      ],
      "metadata": {
        "id": "hdtivp5EmnfD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code for metrics"
      ],
      "metadata": {
        "id": "kjzZ0VuHNMQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 Score\n",
        "# Calculates the harmonic mean of precision and recall based on word overlap.\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def compute_f1_score(predictions, references):\n",
        "    preds = [\" \".join(p.split()) for p in predictions]\n",
        "    refs = [\" \".join(r.split()) for r in references]\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(refs, preds, average=\"macro\")\n",
        "    return f1 * 100\n",
        "\n",
        "# Exact Match (EM)\n",
        "# Measures the percentage of predictions that exactly match the references.\n",
        "\n",
        "def compute_exact_match(predictions, references):\n",
        "    return sum([p.strip() == r.strip() for p, r in zip(predictions, references)]) / len(references) * 100"
      ],
      "metadata": {
        "id": "ItH5_1tNl5-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(predictions, references, bleurt_metric, rouge_metric):\n",
        "    \"\"\"\n",
        "    Evaluate predictions using BLEURT, ROUGE, Exact Match, and F1 Score.\n",
        "\n",
        "    Args:\n",
        "        predictions (list): List of model-generated answers.\n",
        "        references (list): List of ground-truth answers.\n",
        "        bleurt_metric (evaluate.Metric): BLEURT metric instance.\n",
        "        rouge_metric (evaluate.Metric): ROUGE metric instance.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing BLEURT, ROUGE, EM, and F1 scores.\n",
        "    \"\"\"\n",
        "    # BLEURT scores\n",
        "    bleurt_results = bleurt_metric.compute(predictions=predictions, references=references)\n",
        "    bleurt_scores = bleurt_results[\"scores\"]\n",
        "    avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
        "    rouge_l_f1 = rouge_results[\"rougeLsum\"]\n",
        "\n",
        "    # Exact Match (EM) Score\n",
        "    em_score = compute_exact_match(predictions, references)\n",
        "\n",
        "    # F1 Score\n",
        "    f1_score = compute_f1_score(predictions, references)\n",
        "\n",
        "    return {\n",
        "        \"BLEURT Average\": avg_bleurt,\n",
        "        \"ROUGE-L F1\": rouge_l_f1 * 100,  # Convert to percentage\n",
        "        \"Exact Match\": em_score,\n",
        "        \"F1 Score\": f1_score,\n",
        "    }\n",
        "\n",
        "def generate_predictions(model, tokenizer, dataset, max_length=256):\n",
        "    \"\"\"\n",
        "    Generate predictions for evaluation.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    references = []\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        # Tokenize input dynamically\n",
        "        inputs = tokenizer(item[\"input\"], return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "        # inputs = tokenizer(item[\"input\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "        # Generate predictions\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(item[\"target\"])\n",
        "\n",
        "    return predictions, references"
      ],
      "metadata": {
        "id": "UphO_BpD2go0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "results for combinations of models and given context"
      ],
      "metadata": {
        "id": "Ic5RZ28fNPJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_context_model.to(\"cuda\")\n",
        "context_model.to(\"cuda\")\n",
        "\n",
        "#short hand no context to NC, context to C, no context trained model to NCM, context trained model CM\n",
        "NCM_given_NC_predictions, NCM_given_NC_references = generate_predictions(no_context_model, tokenizer, no_context_dataset)\n",
        "NCM_given_C_predictions, NCM_given_C_references = generate_predictions(no_context_model, tokenizer, context_dataset)\n",
        "CM_given_NC_predictions, CM_given_NC_references = generate_predictions(context_model, tokenizer, no_context_dataset)\n",
        "CM_given_C_predictions, CM_given_C_references = generate_predictions(context_model, tokenizer, context_dataset)\n",
        "\n",
        "# Evaluate the models\n",
        "NCM_given_NC_results = evaluate_model(NCM_given_NC_predictions, NCM_given_NC_references, bleurt_metric, rouge_metric)\n",
        "NCM_given_C_results = evaluate_model(NCM_given_C_predictions, NCM_given_C_references, bleurt_metric, rouge_metric)\n",
        "CM_given_NC_results = evaluate_model(CM_given_NC_predictions, CM_given_NC_references, bleurt_metric, rouge_metric)\n",
        "CM_given_C_results = evaluate_model(CM_given_C_predictions, CM_given_C_references, bleurt_metric, rouge_metric)\n",
        "\n",
        "print(\"No Context Model Given No Context Results:\")\n",
        "print(f\"BLEURT Average Score: {NCM_given_NC_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {NCM_given_NC_results['ROUGE-L F1']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"No Context Model Given Context Results:\")\n",
        "print(f\"BLEURT Average Score: {NCM_given_C_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {NCM_given_C_results['ROUGE-L F1']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"Context Model Given No Context Results:\")\n",
        "print(f\"BLEURT Average Score: {CM_given_NC_predictions['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {CM_given_NC_predictions['ROUGE-L F1']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"Context Model Given Context Results:\")\n",
        "print(f\"BLEURT Average Score: {CM_given_C_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {CM_given_C_results['ROUGE-L F1']:.2f}%\")\n",
        "print()"
      ],
      "metadata": {
        "id": "GqKRxJ0LMJyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#typo in last cell, to not run for over an hour again, i just made this cell\n",
        "print(\"No Context Model Given No Context Results:\")\n",
        "print(f\"BLEURT Average Score: {NCM_given_NC_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {NCM_given_NC_results['ROUGE-L F1']:.2f}%\")\n",
        "print(f\"Exact Match Score: {NCM_given_NC_results['Exact Match']:.2f}%\")\n",
        "print(f\"F1 Score: {NCM_given_NC_results['F1 Score']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"No Context Model Given Context Results:\")\n",
        "print(f\"BLEURT Average Score: {NCM_given_C_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {NCM_given_C_results['ROUGE-L F1']:.2f}%\")\n",
        "print(f\"Exact Match Score: {NCM_given_C_results['Exact Match']:.2f}%\")\n",
        "print(f\"F1 Score: {NCM_given_C_results['F1 Score']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"Context Model Given No Context Results:\")\n",
        "print(f\"BLEURT Average Score: {CM_given_NC_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {CM_given_NC_results['ROUGE-L F1']:.2f}%\")\n",
        "print(f\"Exact Match Score: {CM_given_NC_results['Exact Match']:.2f}%\")\n",
        "print(f\"F1 Score: {CM_given_NC_results['F1 Score']:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"Context Model Given Context Results:\")\n",
        "print(f\"BLEURT Average Score: {CM_given_C_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {CM_given_C_results['ROUGE-L F1']:.2f}%\")\n",
        "print(f\"Exact Match Score: {CM_given_C_results['Exact Match']:.2f}%\")\n",
        "print(f\"F1 Score: {CM_given_C_results['F1 Score']:.2f}%\")\n",
        "print()"
      ],
      "metadata": {
        "id": "_B3kEv5_7evJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained results"
      ],
      "metadata": {
        "id": "-ryxXeQgM-9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "t5.to(\"cuda\")\n",
        "# no_context_model.to(\"cuda\")\n",
        "t5_predictions, t5_references = generate_predictions(t5, tokenizer, no_context_dataset)\n",
        "t5_results = evaluate_model(t5_predictions, t5_references, bleurt_metric, rouge_metric)"
      ],
      "metadata": {
        "id": "y9yEzn8i2bn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context Model Given Context Results:\")\n",
        "print(f\"BLEURT Average Score: {t5_results['BLEURT Average']:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {t5_results['ROUGE-L F1']:.2f}%\")\n",
        "print(f\"Exact Match Score: {t5_results['Exact Match']:.2f}%\")\n",
        "print(f\"F1 Score: {t5_results['F1 Score']:.2f}%\")\n",
        "print()"
      ],
      "metadata": {
        "id": "xf-lROyo6HPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Human Eval*\n",
        "We additionally hand evaluated some results for more data.\n",
        "- Here is a link to [the notebook that made the spreadsheet](https://colab.research.google.com/drive/166EPUJBX61T4QMm7JF3Jt2KOtlePo6dv?usp=sharing)\n",
        "- Here is a link to [the google spreadsheet we filled in](https://docs.google.com/spreadsheets/d/1Y30Vr81zzLvb_XWCEKM1OiXPq_ToM3CVctbVE6fEKME/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "RAI8GJvanwWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Demo*"
      ],
      "metadata": {
        "id": "b9DjdPVTxru8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import textwrap # for nice formatting"
      ],
      "metadata": {
        "id": "J1BB974C5uT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load Stonybrook TellMeWhy and find the bad Morroco example\n",
        "tmw_dataset = load_dataset(\"StonyBrookNLP/tellmewhy\")   # provided dataset"
      ],
      "metadata": {
        "id": "RTSlqOSM6jvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contains_morocco = lambda data: True if \"Morocco\" in data[\"answer\"] else False\n",
        "morocco_example = list(filter(contains_morocco, tmw_dataset[\"train\"]))[0]\n",
        "\n",
        "def print_par_neatly(str):\n",
        "  print(textwrap.fill(str, width=80))\n",
        "\n",
        "print(\"Morocco example we mentioned at the presentation:\")\n",
        "print(f\"\\n{'Narrative':10}: \")\n",
        "print_par_neatly(morocco_example['narrative'])\n",
        "print(f\"\\n{'Question':10}: {morocco_example['question']}\")\n",
        "print(f\"\\n{'Answer':10}: {morocco_example['answer']}\")\n"
      ],
      "metadata": {
        "id": "jxJXCHDSxuBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get random example\n",
        "SEED = 1014 # ask for a number\n",
        "random_data = no_context_dataset.shuffle(seed=SEED)[0]\n",
        "\n",
        "# print data\n",
        "print(\"Shuffled data example:\")\n",
        "print(f\"\\n{'Concatted input'}: \")\n",
        "print_par_neatly(random_data['input'])\n",
        "print(f\"\\n{'Target'}: {random_data['target']}\")\n",
        "\n",
        "# generate output from both our models and a pretrained t5-small\n",
        "input_ids = tokenizer(random_data['input'], return_tensors=\"pt\")[\"input_ids\"]\n",
        "context_output = tokenizer.decode(context_model.generate(input_ids)[0], skip_special_tokens=True)\n",
        "no_context_output = tokenizer.decode(no_context_model.generate(input_ids)[0], skip_special_tokens=True)\n",
        "t5_output = tokenizer.decode(t5.generate(input_ids)[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nPretrained T5-small output:\")\n",
        "print(t5_output)\n",
        "print()\n",
        "\n",
        "print(\"No context model output:\")\n",
        "print(no_context_output)\n",
        "print()\n",
        "\n",
        "print(\"Context model output:\")\n",
        "print(context_output)\n",
        "print()"
      ],
      "metadata": {
        "id": "nzAjEbuN008t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}